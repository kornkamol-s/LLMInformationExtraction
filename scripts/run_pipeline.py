import argparse, csv, logging 
import pandas as pd
from tools.OpenAIConnection import OpenAIConnection
from tools.utils import find_pdf_files, get_filtered_file
from scripts.processing import context_extractor
from config import config

# Set up logging configuration with timestamps
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')


def main(args):    
    """
    Main function to orchestrate the information extraction pipeline.
    """
    output = f'data/inference/output/{args.m.split('::')[-1]}.csv'

    # Connect to OpenAI
    openai = OpenAIConnection(args.m)
    
    # If ids are provided, process only those given ids.
    # Otherwise, process entire folder
    # Also, filter out all the processed ids
    pdf_files = get_filtered_file(find_pdf_files(args.input), args.ids, output)

    if pdf_files:

        # Extract relevant pargraphs of each specific question, and save in local directory
        logging.info('Step 1: Context Extraction')
        logging.info('==================================')
        context_extractor.main(args)

        # Read those extracted contexts to dataframe for extracting information using GPT
        context_df = pd.read_csv(args.output, encoding='utf-8')

        # Map specific question to each type of information
        context_df['question'] = context_df['section_category'].map(config.QUESTION_MAPPING)

        # Transform into final prompts
        context_df['prompt'] = context_df.apply(lambda row: _transform_record(row), axis=1)

        logging.info('Step 2: Entity Extraction using GPT')
        logging.info('==================================')
        # Iteratively process each input file
        for i, f in enumerate(pdf_files):
            logging.info(f"[{i+1}/{len(pdf_files)}]: Processing {f}")

            # Filter only focusing project
            df = context_df[context_df['filename']==f]

            # Save model's responses to local directory
            _save_response(openai, args.m, df, output, temperature=0, max_token=1000)


def _save_response(openai, model, prompts, output, temperature=0, max_token=1000):
    """
    Save responses from the OpenAI model to a CSV file.

    Parameters:
        openai (OpenAIConnection): Connection to the OpenAI API.
        model (str): The selected OpenAI model.
        prompts (DataFrame): DataFrame containing prompts for the model.
        output (str): File path for the output CSV.
        temperature (float, optional): Controls randomness of output (default: 0).
        max_token (int, optional): Max number of tokens for the model's response (default: 1000).
    """
    with open(output, mode='a', encoding='utf-8', newline='') as f:
        # Initialise dictwriter to write csv file
        writer = csv.DictWriter(f, fieldnames=['id', 'filename', 'type', 'response'])

        # Write header if no previous record exists
        if f.tell() == 0:
            writer.writeheader()

        # Iteratively loop through each prompt
        for _, row in prompts.iterrows():
            
            # Generate model's response
            response = _get_model_response(openai, row['prompt'], model, temperature, max_token)
            row = {
                'id': row['id'],
                'filename': row['filename'],
                'type': row['section_category'],
                'response': response
            }

            # Write records to local file
            writer.writerow(row)


def _get_model_response(openai, prompt, model, temperature, max_token):
    """
    Get a response from the model based on the provided prompt.

    Parameters:
        openai (OpenAIConnection): Connection to the OpenAI API.
        prompt (str): The prompt to evaluate.
        model (str): The selected OpenAI model.
        temperature (float): Controls randomness of output.
        max_token (int): Max number of tokens for the response.

    Returns:
        str: The response generated by the OpenAI model.
    """
    return openai._evaluate_model(model, prompt, temperature, max_token)


def _transform_record(row):
    """
    Transform a row of context data into a structured prompt for the OpenAI model.

    Parameters:
        row (Series): A single row of information.

    Returns:
        dict: A formatted dictionary for the OpenAI API.
    """
    return [
            {
                "role": "system",
                "content": "You are tasked with extracting relevant information from the provided context to answer the following question. If any key information is missing, omit that key from response. If No relevant information found in context is found, do not return anything."
            },
            {
                "role": "user",
                "content": f"Question: {row['question']}\n\nContext: {row['context']}"
            }
        ]


def _setup_args():
    """
    Set up command-line arguments.

    Returns:
        argparse: The parsed arguments.
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('ids', type=int, nargs='*', help='Project IDs which documents will be processed')
    parser.add_argument('--m', type=str, help='Selected Model')
    parser.add_argument('--input', type=str, default='data/inference/input', nargs='?', help='Input Folder')
    parser.add_argument('--output', type=str, default='data/inference/intermediate/context.csv', nargs='?', help='Output File to store extracted context')
    args = parser.parse_args()

    return args


if __name__ == "__main__":
    # Set up command-line arguments
    args = _setup_args()

    # Execute the main function with the parsed arguments
    main(args)